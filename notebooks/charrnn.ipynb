{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "params = {\n",
    "    'batch_size': 128,\n",
    "    'text_iter_step': 25,\n",
    "    'seq_len': 200,\n",
    "    'hidden_dim': 128,\n",
    "    'n_layers': 2,\n",
    "    'beam_width': 5,\n",
    "    'display_step': 10,\n",
    "    'generate_step': 100,\n",
    "    'clip_norm': 5.0,\n",
    "}\n",
    "\n",
    "def parse_text(file_path):\n",
    "    with open(file_path) as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    char2idx = {c: i+3 for i, c in enumerate(set(text))}\n",
    "    char2idx['<pad>'] = 0\n",
    "    char2idx['<start>'] = 1\n",
    "    char2idx['<end>'] = 2\n",
    "    \n",
    "    ints = np.array([char2idx[char] for char in list(text)])\n",
    "    return ints, char2idx\n",
    "\n",
    "def next_batch(ints):\n",
    "    len_win = params['seq_len'] * params['batch_size']\n",
    "    for i in range(0, len(ints)-len_win, params['text_iter_step']):\n",
    "        clip = ints[i: i+len_win]\n",
    "        yield clip.reshape([params['batch_size'], params['seq_len']])\n",
    "        \n",
    "def input_fn(ints):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: next_batch(ints), tf.int32, tf.TensorShape([None, params['seq_len']]))\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def start_sent(x):\n",
    "    _x = tf.fill([tf.shape(x)[0], 1], params['char2idx']['<start>'])\n",
    "    return tf.concat([_x, x], 1)\n",
    "\n",
    "def end_sent(x):\n",
    "    _x = tf.fill([tf.shape(x)[0], 1], params['char2idx']['<end>'])\n",
    "    return tf.concat([x, _x], 1)\n",
    "\n",
    "def cell_fn():\n",
    "    return tf.nn.rnn_cell.ResidualWrapper(\n",
    "        tf.nn.rnn_cell.GRUCell(params['hidden_dim'],\n",
    "            kernel_initializer=tf.orthogonal_initializer()))\n",
    "  \n",
    "def multi_cell_fn():\n",
    "    return tf.nn.rnn_cell.MultiRNNCell([cell_fn() for _ in range(params['n_layers'])])\n",
    "\n",
    "def clip_grads(loss):\n",
    "    variables = tf.trainable_variables()\n",
    "    grads = tf.gradients(loss, variables)\n",
    "    clipped_grads, _ = tf.clip_by_global_norm(grads, params['clip_norm'])\n",
    "    return zip(clipped_grads, variables)\n",
    "\n",
    "def forward(inputs, is_training):\n",
    "    if is_training:\n",
    "        batch_sz = tf.shape(inputs)[0]\n",
    "        \n",
    "        with tf.variable_scope('main', reuse=False):\n",
    "            embedding = tf.get_variable('lookup_table', [params['vocab_size'], params['hidden_dim']])\n",
    "            cells = multi_cell_fn()\n",
    "            \n",
    "            helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                inputs = tf.nn.embedding_lookup(embedding, inputs),\n",
    "                sequence_length = tf.count_nonzero(inputs, 1, dtype=tf.int32))\n",
    "\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell = cells,\n",
    "                helper = helper,\n",
    "                initial_state = cells.zero_state(batch_sz, tf.float32),\n",
    "                output_layer = tf.layers.Dense(params['vocab_size']))\n",
    "\n",
    "            decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder = decoder)\n",
    "\n",
    "            logits = decoder_output.rnn_output\n",
    "            return logits\n",
    "    \n",
    "    if not is_training:\n",
    "        with tf.variable_scope('main', reuse=True):\n",
    "            cells = multi_cell_fn()\n",
    "            \n",
    "            decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                cell = cells,\n",
    "                embedding = tf.get_variable('lookup_table'),\n",
    "                start_tokens = tf.tile(tf.constant(\n",
    "                    [params['char2idx']['<start>']], dtype=tf.int32), [1]),\n",
    "                end_token = params['char2idx']['<end>'],\n",
    "                initial_state = tf.contrib.seq2seq.tile_batch(\n",
    "                    cells.zero_state(1, tf.float32), params['beam_width']),\n",
    "                beam_width = params['beam_width'],\n",
    "                output_layer = tf.layers.Dense(params['vocab_size'], _reuse=True))\n",
    "\n",
    "            decoder_out, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder = decoder,\n",
    "                maximum_iterations = params['seq_len'])\n",
    "\n",
    "            predict = decoder_out.predicted_ids[:, :, 0]\n",
    "            return predict\n",
    "\n",
    "ints, params['char2idx'] = parse_text('../temp/anna.txt')\n",
    "params['vocab_size'] = len(params['char2idx'])\n",
    "params['idx2char'] = {i: c for c, i in params['char2idx'].items()}\n",
    "print('Vocabulary size:', params['vocab_size'])\n",
    "\n",
    "ops = {}\n",
    "X = input_fn(ints)\n",
    "\n",
    "logits = forward(start_sent(X), is_training=True)\n",
    "\n",
    "ops['global_step'] = tf.Variable(0, trainable=False)\n",
    "\n",
    "targets = end_sent(X)\n",
    "ops['loss'] = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(\n",
    "    logits = logits,\n",
    "    targets = targets,\n",
    "    weights = tf.to_float(tf.ones_like(targets))))\n",
    "\n",
    "ops['train'] = tf.train.AdamOptimizer().apply_gradients(\n",
    "    clip_grads(ops['loss']), global_step=ops['global_step'])\n",
    "\n",
    "ops['generate'] = forward(None, is_training=False)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "while True:\n",
    "    try:\n",
    "        _, step, loss = sess.run([ops['train'], ops['global_step'], ops['loss']])\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "    else:\n",
    "        if step % params['display_step'] == 0 or step == 1:\n",
    "            print(\"Step %d | Loss %.3f\" % (step, loss))\n",
    "        if step % params['generate_step'] == 0 and step > 1:\n",
    "            ints = sess.run(ops['generate'])[0]\n",
    "            print('\\n'+''.join([params['idx2char'][i] for i in ints])+'\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
